{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center><font size = 5>Blah, blah, blah</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Blah, blah, blah, blah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "1. [Data Cleaning](#0)<br>\n",
    "2. [Data Preprocessing](#1)<br>\n",
    "3. [Visualizing Data](#2)<br>\n",
    "3. [Modeling](#3)<br>\n",
    "4. [Blah, blah, blah](#4) <br>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://md01.rcc.local:4053\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0-cdh6.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f31050f8278>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import IntegerType, StructType, StructField, StringType\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Final_project').getOrCreate()\n",
    "#change configuration settings on Spark \n",
    "conf = spark.sparkContext._conf.setAll([('spark.executor.memory', '5g'), ('spark.app.name', 'Spark Updated Conf'), ('spark.executor.cores', '6'), ('spark.cores.max', '4'), ('spark.driver.memory','8g')])\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.port', '35340'),\n",
       " ('spark.eventLog.enabled', 'true'),\n",
       " ('spark.yarn.jars',\n",
       "  'local:/opt/cloudera/parcels/CDH-6.3.0-1.cdh6.3.0.p0.1279813/lib/spark/jars/*,local:/opt/cloudera/parcels/CDH-6.3.0-1.cdh6.3.0.p0.1279813/lib/spark/hive/*'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1577383759214_3746'),\n",
       " ('spark.yarn.appMasterEnv.MKL_NUM_THREADS', '1'),\n",
       " ('spark.sql.queryExecutionListeners',\n",
       "  'com.cloudera.spark.lineage.NavigatorQueryListener'),\n",
       " ('spark.executor.memory', '5g'),\n",
       " ('spark.lineage.log.dir', '/var/log/spark/lineage'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'md01.rcc.local,md02.rcc.local'),\n",
       " ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.10.7-src.zip:/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip<CPS>/opt/cloudera/parcels/CDH-6.3.0-1.cdh6.3.0.p0.1279813/lib/spark/python/lib/py4j-0.10.7-src.zip<CPS>/opt/cloudera/parcels/CDH-6.3.0-1.cdh6.3.0.p0.1279813/lib/spark/python/lib/pyspark.zip'),\n",
       " ('spark.yarn.historyServer.address', 'http://hd01.rcc.local:18088'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.network.crypto.enabled', 'false'),\n",
       " ('spark.executorEnv.MKL_NUM_THREADS', '1'),\n",
       " ('spark.ui.enabled', 'true'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.executor.extraLibraryPath',\n",
       "  '/opt/cloudera/parcels/CDH-6.3.0-1.cdh6.3.0.p0.1279813/lib/hadoop/lib/native'),\n",
       " ('spark.dynamicAllocation.schedulerBacklogTimeout', '1'),\n",
       " ('spark.driver.appUIAddress', 'http://md01.rcc.local:4053'),\n",
       " ('spark.yarn.config.gatewayPath', '/opt/cloudera/parcels'),\n",
       " ('spark.extraListeners', 'com.cloudera.spark.lineage.NavigatorAppListener'),\n",
       " ('spark.port.maxRetries', '60'),\n",
       " ('spark.sql.warehouse.dir', '/user/hive/warehouse'),\n",
       " ('spark.app.name', 'Spark Updated Conf'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.driver.log.persistToDfs.enabled', 'true'),\n",
       " ('spark.yarn.config.replacementPath', '{{HADOOP_COMMON_HOME}}/../../..'),\n",
       " ('spark.executorEnv.OPENBLAS_NUM_THREADS', '1'),\n",
       " ('spark.driver.extraLibraryPath',\n",
       "  '/opt/cloudera/parcels/CDH-6.3.0-1.cdh6.3.0.p0.1279813/lib/hadoop/lib/native'),\n",
       " ('spark.executor.instances', '32'),\n",
       " ('spark.ui.killEnabled', 'true'),\n",
       " ('spark.cores.max', '4'),\n",
       " ('spark.eventLog.dir', 'hdfs://nameservice1/user/spark/applicationHistory'),\n",
       " ('spark.dynamicAllocation.executorIdleTimeout', '60'),\n",
       " ('spark.io.encryption.enabled', 'false'),\n",
       " ('spark.authenticate', 'false'),\n",
       " ('spark.driver.memory', '8g'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.RM_HA_URLS',\n",
       "  'md01.rcc.local:8088,md02.rcc.local:8088'),\n",
       " ('spark.shuffle.service.enabled', 'true'),\n",
       " ('spark.yarn.historyServer.allowTracking', 'true'),\n",
       " ('spark.yarn.appMasterEnv.OPENBLAS_NUM_THREADS', '1'),\n",
       " ('spark.shuffle.service.port', '7337'),\n",
       " ('spark.lineage.enabled', 'true'),\n",
       " ('spark.app.id', 'application_1577383759214_3746'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.driver.host', 'md01.rcc.local'),\n",
       " ('spark.executor.cores', '6'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.yarn.am.extraLibraryPath',\n",
       "  '/opt/cloudera/parcels/CDH-6.3.0-1.cdh6.3.0.p0.1279813/lib/hadoop/lib/native'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://md01.rcc.local:8088/proxy/application_1577383759214_3746,http://md02.rcc.local:8088/proxy/application_1577383759214_3746'),\n",
       " ('spark.dynamicAllocation.minExecutors', '0'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.dynamicAllocation.enabled', 'true'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.log.dfsDir', '/user/spark/driverLogs')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datapath\n",
    "path_comments = \"/user/rroongseang/bigdata/parquet/comments_parquet/\"\n",
    "path_users = \"/user/rroongseang/bigdata/users/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data\n",
    "df = sqlContext.read.parquet(path_comments)\n",
    "users = spark.read.csv(path_users+\"RA.2019-09.csv\", inferSchema=True, header=True)\n",
    "botusers = sqlContext.read.csv(path_users+'bot_userdata.csv',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning <a id=\"0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86633134"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----------------------+-----------------+----+----------------+-----------+-------------+--------+------+---+-------+--------+---------+------------+-----+------------+---------+------------+--------+\n",
      "|archived|author|author_flair_css_class|author_flair_text|body|controversiality|created_utc|distinguished|   downs|gilded| id|link_id|    name|parent_id|retrieved_on|score|score_hidden|subreddit|subreddit_id|     ups|\n",
      "+--------+------+----------------------+-----------------+----+----------------+-----------+-------------+--------+------+---+-------+--------+---------+------------+-----+------------+---------+------------+--------+\n",
      "|66401016|     0|              75695490|         75551029|   0|               0|          0|     84850803|84468358|     0|  0|      0|84468358|        0|       18219|    0|    76923215|        0|           0|69620202|\n",
      "+--------+------+----------------------+-----------------+----+----------------+-----------+-------------+--------+------+---+-------+--------+---------+------------+-----+------------+---------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping several columns with missing and irrelevant data\n",
    "#subreddit/subreddit_id is the same for all comments since we are only looking in r/politics.\n",
    "#author flairs are texts + images next to a username that shows up when they post a comment. Most users don't use flairs\n",
    "#name is a unique identifier that is mostly null.\n",
    "#id is a unique identifier for the commment and does not add any value to analysis\n",
    "df = df.drop('archived','author_flair_css_class','author_flair_text',\n",
    "             'subreddit','subreddit_id','name','score_hidden','id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert column types to integers and timestamps\n",
    "\n",
    "df = df.withColumn(\"ups\", df[\"ups\"].cast(IntegerType())) \n",
    "df = df.withColumn(\"downs\", df[\"downs\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"gilded\", df[\"gilded\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"score\", df[\"score\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"retrieved_on\", df[\"retrieved_on\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"controversiality\", df[\"controversiality\"].cast(IntegerType()))\n",
    "df = df.withColumn('created_utc',df[\"created_utc\"].cast(IntegerType()))\n",
    "df = df.withColumn('retrieved_on',df[\"retrieved_on\"].cast(IntegerType()))\n",
    "\n",
    "\n",
    "df = df.withColumn('created_utc',to_timestamp(df[\"created_utc\"]))\n",
    "df = df.withColumn('retrieved_on',to_timestamp(df[\"retrieved_on\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- controversiality: integer (nullable = true)\n",
      " |-- created_utc: timestamp (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- downs: integer (nullable = true)\n",
      " |-- gilded: integer (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- retrieved_on: timestamp (nullable = true)\n",
      " |-- score: integer (nullable = true)\n",
      " |-- ups: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill upvote and downvote null values with zeros\n",
    "df = df.fillna({ 'ups':0, 'downs':0 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename a user and botuser columns to match for union of dataframe\n",
    "users = users.selectExpr('id',\"name as username\", \"created_utc as acct_creation\",\n",
    "                         \"updated_on\",\"comment_karma\",\"link_karma\")\n",
    "\n",
    "botusers = botusers.selectExpr('username',\"post_karma as link_karma\", \"comment_karma\",\n",
    "                                 \"cake_day as acct_creation\",\"is_bot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "#Convert account creation and updated on to timestampps\n",
    "users = users.withColumn('acct_creation',to_timestamp(users[\"acct_creation\"]))\n",
    "users = users.withColumn('updated_on',to_timestamp(users[\"updated_on\"]))\n",
    "\n",
    "#Add column is_bot to users dataframe\n",
    "users = users.withColumn('is_bot',f.lit('False'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cast link karma and comment karma as integer type\n",
    "#Convert account creation and updated on to timestamps\n",
    "botusers = botusers.withColumn('link_karma',botusers[\"link_karma\"].cast(IntegerType()))\n",
    "botusers = botusers.withColumn('comment_karma',botusers[\"comment_karma\"].cast(IntegerType()))\n",
    "botusers = botusers.withColumn('acct_creation',botusers[\"acct_creation\"].cast(IntegerType()))\n",
    "botusers = botusers.withColumn('acct_creation',to_timestamp(botusers[\"acct_creation\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove rows from user df that exists in botusers\n",
    "users = users.join(botusers, users.username==botusers.username, \"leftanti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append users dataframe with botusers dataframe \n",
    "all_users = users.select('username','acct_creation','comment_karma','link_karma','is_bot')\\\n",
    "            .union(botusers.select('username','acct_creation','comment_karma','link_karma','is_bot'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- username: string (nullable = true)\n",
      " |-- acct_creation: timestamp (nullable = true)\n",
      " |-- comment_karma: integer (nullable = true)\n",
      " |-- link_karma: integer (nullable = true)\n",
      " |-- is_bot: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_users.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+-------------+----------+------+\n",
      "|     username|      acct_creation|comment_karma|link_karma|is_bot|\n",
      "+-------------+-------------------+-------------+----------+------+\n",
      "|MiraranaMogra|2015-05-14 09:58:34|          103|      4902|  True|\n",
      "+-------------+-------------------+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking to see if one of the Russian banned accts is in the users list\n",
    "all_users.filter(all_users.username == 'MiraranaMogra').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining data frames\n",
    "df_raw = df.join(all_users, df.author == users.username, 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69638017"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of comments matched with users dataframe\n",
    "df_raw.filter(users.username.isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69638017"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop rows where comments authors do not appear in users dataframe\n",
    "df_raw = df_raw.filter(df_raw.is_bot.isNotNull())\n",
    "df_raw.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of banned account comments in dataframe\n",
    "df_raw.filter(df_raw.is_bot == 'True').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing <a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "| <p align=\"left\">**Step**</p>| <p align=\"left\">**Action**</p>|<p align=\"left\">**Example**</p>|\n",
    "|---|---|---|\n",
    "| <p align=\"left\">Step 0</p> | <p align=\"left\">Raw Comment</p>  | <p align=\"left\">\"Almost 1/3 of registered voters in CA are republican, though...\"</p>  |\n",
    "| <p align=\"left\">Step 1</p> | <p align=\"left\">Put text into lower case</p>  | <p align=\"left\">\"almost 1/3 of registered voters in ca are republican, though...\"</p>  |\n",
    "| <p align=\"left\">Step 2</p>  | <p align=\"left\">Remove Non-Text Characters</p>  | <p align=\"left\">\"almost of registered voters in ca are republican though\"</p> |\n",
    "| <p align=\"left\">Step 3</p>  | <p align=\"left\">Tokenize Text</p>  | <p align=\"left\">[\"almost\", \"of\", \"registered\", \"voters\", \"in\", \"ca\", \"are\", \"republican\", \"though\"]</p> |\n",
    "| <p align=\"left\">Step 4</p>  | <p align=\"left\">Remove Stopwords</p>  | <p align=\"left\">[\"almost\", \"registered\", \"voters\", \"ca\", \"republican\"]</p> |\n",
    "| <p align=\"left\">Step 5</p>  | <p align=\"left\">Stem using Porter Stemmer</p>  | <p align=\"left\">[\"almost\", \"regist\", \"voter\", \"ca\", \"republican\"]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting comment posted datetime to new column format HHMMSS\n",
    "df_raw = df_raw.withColumn('created_time',concat(format_string(\"%02d\",hour(df_raw['created_utc'])),\n",
    "                                                     format_string(\"%02d\",minute(df_raw['created_utc'])),\n",
    "                                                     format_string(\"%02d\",second(df_raw['created_utc']))))\n",
    "df_raw = df_raw.withColumn('created_time',df_raw['created_time'].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- controversiality: integer (nullable = true)\n",
      " |-- created_utc: timestamp (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- downs: integer (nullable = false)\n",
      " |-- gilded: integer (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- retrieved_on: timestamp (nullable = true)\n",
      " |-- score: integer (nullable = true)\n",
      " |-- ups: integer (nullable = false)\n",
      " |-- username: string (nullable = true)\n",
      " |-- acct_creation: timestamp (nullable = true)\n",
      " |-- comment_karma: integer (nullable = true)\n",
      " |-- link_karma: integer (nullable = true)\n",
      " |-- is_bot: string (nullable = true)\n",
      " |-- created_time: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def getInterval(time):\n",
    "    start = int(time)\n",
    "    return str(start)+\"-\"+str(start+1)\n",
    "\n",
    "getIntervalUdf = udf(getInterval,StringType())\n",
    "\n",
    "df_raw = df_raw.withColumn('year', year(col('created_utc')))\n",
    "df_raw = df_raw.withColumn('month', month(col('created_utc')))\n",
    "df_raw = df_raw.withColumn('time', hour(col('created_utc')))\n",
    "\n",
    "df_raw = df_raw.withColumn(\"interval\",getIntervalUdf(\"time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove formatting characters\n",
    "df_raw = df_raw.withColumn('body_vec', f.regexp_replace('body', \"[^a-zA-Z0-9\\\\s]\", \"\"))\n",
    "\n",
    "#Next, we will put all letters into lower-case\n",
    "df_raw = df_raw.withColumn('body_vec', lower(col('body_vec')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+----------------+-------------------+-------------+-----+------+---------+----------+-------------------+-----+---+------------+-------------------+-------------+----------+------+------------+----+-----+----+--------+--------------------+\n",
      "|      author|                body|controversiality|        created_utc|distinguished|downs|gilded|  link_id| parent_id|       retrieved_on|score|ups|    username|      acct_creation|comment_karma|link_karma|is_bot|created_time|year|month|time|interval|            body_vec|\n",
      "+------------+--------------------+----------------+-------------------+-------------+-----+------+---------+----------+-------------------+-----+---+------------+-------------------+-------------+----------+------+------------+----+-----+----+--------+--------------------+\n",
      "|---JustMe---|Yes. $16 million ...|               0|2016-05-09 17:52:25|         null|    0|     0|t3_4ikml4|t1_d2z41q9|2016-06-12 10:36:52|    4|  4|---JustMe---|2014-02-17 23:55:37|         8488|      1183| False|      175225|2016|    5|  17|   17-18|yes 16 million to...|\n",
      "|---JustMe---|[Straight from th...|               0|2016-05-09 18:26:23|         null|    0|     0|t3_4ik8zx|t1_d2z6x6y|2016-06-12 10:46:43|    6|  6|---JustMe---|2014-02-17 23:55:37|         8488|      1183| False|      182623|2016|    5|  18|   18-19|straight from the...|\n",
      "|---JustMe---|&gt;Tell me again...|               0|2016-05-09 18:34:25|         null|    0|     0|t3_4ik8zx|t1_d2z8tm3|2016-06-12 10:49:00|    3|  3|---JustMe---|2014-02-17 23:55:37|         8488|      1183| False|      183425|2016|    5|  18|   18-19|gttell me again h...|\n",
      "|---JustMe---|It has appeared t...|               0|2017-04-06 15:15:19|         null|    0|     0|t3_63vdst| t3_63vdst|2017-05-03 18:48:19|    1|  0|---JustMe---|2014-02-17 23:55:37|         8488|      1183| False|      151519|2017|    4|  15|   15-16|it has appeared t...|\n",
      "| -Degaussed-|And me. Working m...|               0|2018-04-20 04:17:43|         null|    0|     0|t3_8djwof|t1_dxo7fok|2018-05-17 01:36:54|    4|  0| -Degaussed-|2015-04-16 13:53:11|        12397|        52| False|       41743|2018|    4|   4|     4-5|and me working my...|\n",
      "+------------+--------------------+----------------+-------------------+-------------+-----+------+---------+----------+-------------------+-----+---+------------+-------------------+-------------+----------+------+------------+----+-----+----+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "#import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#this code courtesy of http://michael-harmon.com/blog/SentimentAnalysisP2.html\n",
    "\n",
    "class PorterStemming(Transformer, HasInputCol, HasOutputCol):\n",
    "    \"\"\"\n",
    "    PosterStemming class using the NLTK Porter Stemmer\n",
    "    \n",
    "    This comes from https://stackoverflow.com/questions/32331848/create-a-custom-transformer-in-pyspark-ml\n",
    "    Adapted to work with the Porter Stemmer from NLTK.\n",
    "    \"\"\"\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, \n",
    "                 inputCol  : str = None, \n",
    "                 outputCol : str = None, \n",
    "                 min_size  : int = None):\n",
    "        \"\"\"\n",
    "        Constructor takes in the input column name, output column name,\n",
    "        plus the minimum legnth of a token (min_size)\n",
    "        \"\"\"\n",
    "        # call Transformer classes constructor since were extending it.\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # set Parameter objects minimum token size\n",
    "        self.min_size = Param(self, \"min_size\", \"\")\n",
    "        self._setDefault(min_size=0)\n",
    "\n",
    "        # set the input keywork arguments\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "        # initialize Stemmer object\n",
    "        self.stemmer  = PorterStemmer()\n",
    "\n",
    "        \n",
    "    @keyword_only\n",
    "    def setParams(self, \n",
    "                  inputCol  : str = None, \n",
    "                  outputCol : str = None, \n",
    "                  min_size  : int = None\n",
    "      ) -> None:\n",
    "        \"\"\"\n",
    "        Function to set the keyword arguemnts\n",
    "        \"\"\"\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "\n",
    "    def _stem_func(self, words  : list) -> list:\n",
    "        \"\"\"\n",
    "        Stemmer function call that performs stemming on a\n",
    "        list of tokens in words and returns a list of tokens\n",
    "        that have meet the minimum length requiremnt.\n",
    "        \"\"\"\n",
    "        # We need a way to get min_size and cannot access it \n",
    "        # with self.min_size\n",
    "        min_size       = self.getMinSize()\n",
    "\n",
    "        # stem that actual tokens by applying \n",
    "        # self.stemmer.stem function to each token in \n",
    "        # the words list\n",
    "        stemmed_words  = map(self.stemmer.stem, words)\n",
    "\n",
    "        # now create the new list of tokens from\n",
    "        # stemmed_words by filtering out those\n",
    "        # that are not of legnth > min_size\n",
    "        filtered_words = filter(lambda x: len(x) > min_size, stemmed_words)\n",
    "\n",
    "        return list(filtered_words)\n",
    "    \n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Transform function is the method that is called in the \n",
    "        MLPipleline.  We have to override this function for our own use\n",
    "        and have it call the _stem_func.\n",
    "\n",
    "        Notice how it takes in a type DataFrame and returns type Dataframe\n",
    "        \"\"\"\n",
    "        # Get the names of the input and output columns to use\n",
    "        out_col       = self.getOutputCol()\n",
    "        in_col        = self.getInputCol()\n",
    "\n",
    "        # create the stemming function UDF by wrapping the stemmer \n",
    "        # method function\n",
    "        stem_func_udf = F.udf(self._stem_func, ArrayType(StringType()))\n",
    "        \n",
    "        # now apply that UDF to the column in the dataframe to return\n",
    "        # a new column that has the same list of words after being stemmed\n",
    "        df2           = df.withColumn(out_col, stem_func_udf(df[in_col]))\n",
    "\n",
    "        return df2\n",
    "  \n",
    "  \n",
    "    def setMinSize(self,value):\n",
    "        \"\"\"\n",
    "        This method sets the minimum size value\n",
    "        for the _paramMap dictionary.\n",
    "        \"\"\"\n",
    "        self._paramMap[self.min_size] = value\n",
    "        return self\n",
    "\n",
    "    def getMinSize(self) -> int:\n",
    "        \"\"\"\n",
    "        This method uses the parent classes (Transformer)\n",
    "        .getOrDefault method to get the minimum\n",
    "        size of a token.\n",
    "        \"\"\"\n",
    "        return self.getOrDefault(self.min_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover, IDF, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"body_vec\", outputCol=\"body_vec_token\")\n",
    "remover = StopWordsRemover(inputCol = \"body_vec_token\", outputCol = \"body_vec_token_nosw\")\n",
    "stemmer = PorterStemming(inputCol = \"body_vec_token_nosw\", outputCol = \"body_vec_cleaned\")\n",
    "hashingTF = HashingTF(inputCol=\"body_vec_cleaned\", outputCol=\"body_vec_tf\", numFeatures=100)\n",
    "idf = IDF(inputCol=\"body_vec_tf\", outputCol=\"body_vec_tfidf\")\n",
    "label_stringIdx = StringIndexer(inputCol = \"is_bot\", outputCol = \"label\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, stemmer, hashingTF,idf, label_stringIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time nlpdf = pipeline.fit(df_raw).transform(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=['controversiality', 'ups', 'downs', 'gilded', \n",
    "                                       'interval', 'comment_karma','link_karma',\n",
    "                                        'score'], outputCol='features')\n",
    "nlpdf = assembler.transform(nlpdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlpdf = df_raw.drop('body_vec')\n",
    "# nlpdf = df_raw.drop('body_vec_token')\n",
    "# nlpdf = df_raw.drop('body_vec_token_nosw')\n",
    "# nlpdf = df_raw.drop('body_vec_cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      body_vec_tfidf|\n",
      "+--------------------+\n",
      "|(100,[1,10,30],[2...|\n",
      "|(100,[30,39,40,44...|\n",
      "|(100,[6,48,49,70,...|\n",
      "|(100,[19,30,38,43...|\n",
      "|(100,[3,4,8,9,10,...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlpdf.select('body_vec_tfidf').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlpdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train and test and ensure label classes are stratified\n",
    "#train_df_notbanned, test_df_notbanned = nlpdf.filter(nlpdf.label==0).sample(False,0.0001,seed=5).randomSplit([0.8, 0.2])\n",
    "train_df_notbanned, test_df_notbanned = nlpdf.filter(nlpdf.label==0).randomSplit([0.8, 0.2])\n",
    "train_df_banned, test_df_banned = nlpdf.filter(nlpdf.label==1).randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unioning training sets and test sets\n",
    "train_df = train_df_notbanned.union(train_df_banned)\n",
    "test_df = test_df_notbanned.union(test_df_banned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_train = train_df.cache()\n",
    "# sample_test = test_df.cache()\n",
    "\n",
    "# print(\"length of training set sample {}\".format(sample_train.count()))\n",
    "# print(\"length of test set {}\".format(sample_test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Data <a id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "\n",
    "stats = nlpdf.groupby(\n",
    "    year('created_utc').alias('year'), \n",
    "    month('created_utc').alias('month')).count()\n",
    "\n",
    "stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_pdf = stats.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_pdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = stats_pdf.sort_values(by=['year', 'month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pdf.reset_index(drop=True)\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into a str with leading 0\n",
    "pdf['month'] = pdf['month'].apply(str)\n",
    "pdf['month'] = pdf['month'].str.zfill(2)\n",
    "\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf['year'] = pdf['year'].apply(str)\n",
    "pdf['year_month'] = pdf['year'] + '-' + pdf['month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "ax = pdf.plot(kind='bar', y=\"count\", x='year_month', align='edge', width=1.0, figsize=(20,10), color='blue', legend=False)\n",
    "ax.grid()\n",
    "plt.title(\"Posts made under topic \\\"Politics\\\"\", size=24)\n",
    "plt.ylabel(\"count\", size=14)\n",
    "plt.xlabel(\"year and month\", size=14)\n",
    "\n",
    "for p in ax.patches:\n",
    "    if p.get_height() > 150000:\n",
    "        ax.annotate(np.round(p.get_height(),decimals=2), (p.get_x()+p.get_width()/2., p.get_height()), ha='center', va='center', xytext=(0, 10), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2016_11 = nlpdf.where(nlpdf.year == 2016).where(nlpdf.month == 11)\n",
    "df_2019_2 = nlpdf.where(nlpdf.year == 2019).where(nlpdf.month == 2)\n",
    "df_2019_9 = nlpdf.where(nlpdf.year == 2019).where(nlpdf.month == 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_contains, col, explode\n",
    "\n",
    "# explode lists and retrieve all of the refined tokens\n",
    "words_list_2016_11 = [x[0] for x in df_2016_11.select(explode('body_vec_token_nosw').alias('body_vec_token_nosw')).collect()]\n",
    "words_list_2019_02 = [x[0] for x in df_2019_2.select(explode('body_vec_token_nosw').alias('body_vec_token_nosw')).collect()]\n",
    "words_list_2019_09 = [x[0] for x in df_2019_9.select(explode('body_vec_token_nosw').alias('body_vec_token_nosw')).collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of total tags\n",
    "len(words_list_2019_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique words\n",
    "words_set = set(words_list_2019_02)\n",
    "len(words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use to get counts\n",
    "import pandas as pd\n",
    "pandas_series_2019_02 = pd.Series(words_list_2019_02)\n",
    "pandas_series_2019_02.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install wordcloud\n",
    "#!conda install -c conda-forge wordcloud==1.4.1 --yes\n",
    "\n",
    "# import package and its set of stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "print ('Wordcloud is installed and imported!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert list to string and generate\n",
    "comments_string_2016_11=(\" \").join(words_list_2016_11)\n",
    "comments_string_2019_02=(\" \").join(words_list_2019_02)\n",
    "comments_string_2019_09=(\" \").join(words_list_2019_09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a word cloud object\n",
    "comments = WordCloud(\n",
    "    background_color='white',\n",
    "    max_words=2000)\n",
    "\n",
    "# generate the word cloud\n",
    "comments.generate(comments_string_2016_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# display the word cloud\n",
    "plt.imshow(comments, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the word cloud\n",
    "comments.generate(comments_string_2019_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the word cloud\n",
    "plt.imshow(comments, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the word cloud\n",
    "comments.generate(comments_string_2019_09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the word cloud\n",
    "plt.imshow(comments, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posts within time interval bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interval_counts = nlpdf.where(nlpdf.year == 2019).groupby(\"year\",\"month\",\"interval\").count()\n",
    "df_interval_counts.sort(desc(\"interval\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling <a id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to run: 8.967144966125488\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "start = time.time()\n",
    "# Set parameters for Logistic Regression\n",
    "lgr = LogisticRegression(maxIter=10, featuresCol = 'body_vec_tfidf', labelCol='label')\n",
    "\n",
    "# Fit the model to the data.\n",
    "lgrm = lgr.fit(train_df)\n",
    "#lgrm = lgr.fit(sample_train)\n",
    "end = time.time()\n",
    "print('time to run: {}'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction accuracy: 0.9626168224299065\n",
      "prediction f1 score: 0.9626168224299065\n",
      "time to run: 2.2132484912872314\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# Given a dataset, predict each point's label, and show the results.\n",
    "predictions = lgrm.transform(test_df)\n",
    "#predictions = lgrm.transform(sample_test)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "print('prediction accuracy: {}'\\\n",
    "          .format(evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})))\n",
    "print('prediction f1 score: {}'\\\n",
    "          .format(evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})))\n",
    "end = time.time()\n",
    "print('time to run: {}'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = predictions.select('label').collect()\n",
    "# y_pred = predictions.select('prediction').collect()\n",
    "# true_list = [result.label for result in y_true]\n",
    "# pred_list = [result.prediction for result in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.98      0.98      0.98       105\n",
      "        1.0       0.00      0.00      0.00         2\n",
      "\n",
      "avg / total       0.96      0.96      0.96       107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# print(classification_report(true_list, pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "# Set parameters for the Random Forest.\n",
    "rfc = RandomForestClassifier(maxDepth=5, numTrees=15, impurity=\"gini\", featuresCol='features',\n",
    "                             labelCol=\"label\")\n",
    "\n",
    "# Fit the model to the data.\n",
    "rfcm = rfc.fit(train_df)\n",
    "\n",
    "# Given a dataset, predict each point's label, and show the results.\n",
    "rfpredictions = rfcm.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction accuracy: 1.0\n",
      "prediction f1 score: 1.0\n"
     ]
    }
   ],
   "source": [
    "print('prediction accuracy: {}'\\\n",
    "          .format(evaluator.evaluate(rfpredictions, {evaluator.metricName: \"accuracy\"})))\n",
    "print('prediction f1 score: {}'\\\n",
    "          .format(evaluator.evaluate(rfpredictions, {evaluator.metricName: \"f1\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = rfpredictions.select('label').collect()\n",
    "# y_pred = rfpredictions.select('prediction').collect()\n",
    "# true_list = [result.label for result in y_true]\n",
    "# pred_list = [result.prediction for result in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00       105\n",
      "        1.0       1.00      1.00      1.00         2\n",
      "\n",
      "avg / total       1.00      1.00      1.00       107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# print(classification_report(true_list, pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blah, blah, blah <a id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark 4G 32e",
   "language": "python",
   "name": "pyspark2_4g32e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
